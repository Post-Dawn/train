# Development Blog 

## 20180720 
Here I'll simply conclude the works I've done the past week.

1. I rewrote the program so that it is more user-friendly. It now takes parameters from the console. You may use the following command
    python main.py --help
And you'll know how to use it. 
2. Speed up of the training process. The original training turns out to be highly inefficient. I tweaked the training code so that the time is reduced by roughly half. Also, the generation of structural graph is now officialy $O(n\log n)$, making it a lot faster than before.
3. Some bug fixes. I realized that the original program may diverge on some environments (for example, it diverged everytime when I run it on school server, but converges very well on my own PC.) It turns out to be some numerical bug of either numpy or pytorch, which I solved by normalizing the graph using a little numerical tricks. 
4. I formally tested the classification efficiency of unlabeled data by splitting labeled data as train and test sets. Only train set is known to the embedding training and the test is used for validation. Logistic regression is used, with scores of averagely 0.94340771,  0.86148948,  0.63636364,  0.72779863 (acc, prec, rec, f1). Positive ratio for train and test is around 0.14. 
5. I experimented with different choice of margin and found out that the margin is not relevant; choices ranging from 1 to 20 gives good separatble embedding. Margin may have other uses, for example, it may be related to the embedding dimiension and convergence speed. I'll look into that. 
6. I used a simple graph label propagation algorithm that operates directly on graphs. Our method outperforms the algorithm significantly. I wonder it is because the method fails when the labels are unbalanced. Anyway, label propagation gives (0.86815415821501019, 0.23529411764705882, 0.033057851239669422, 0.057971014492753624) as acc, precision, recall, and f1. 

## 20180722
I implemented Graph Convolutional Network (proposed by Thomas Kipf et al.) the way described in the paper SEMI-SUPERVISED CLASSIFICATION WITH
GRAPH CONVOLUTIONAL NETWORKS. The best result I've achieved is (0.1643002028397566, 0.1216361679224973, 0.93388429752066116, 0.21523809523809523). It is also possible that it classifies everything as 0, due to the low positive rate. The result is somehow similar to label propagation. Unless my implementation is wrong, I think their model is unfriendly to unbalanced labels and highly fragmented network structure, as is our case.

GCN, on the other hand, achieves very low loss (0.98 f1) on train set. It means that it is able encode in such a way that labeled nodes are linearly separated. However, the unlabeled nodes are not well classified. In additition to unbalanced labels, I think one reason is that our network shows no clear separation between positive and negative users. These users are somehow mixed in a community. Our method, in some sense, drives them apart. Maybe that's how we have a good performance.

I also tried for more times the effect of margin, chosen from (1, 2, 5, 10, 20). Each is done tree times. 

| margin | accuracy | precision | recall | f1 score |
| :-----: | :--------   | :-----   | :---- | :------- |
|1 |0.93610548|  0.84525797|  0.58402204|  0.69011316|
|2 |0.93103448|  0.832669  |  0.54820937|  0.66054379|
|5 |0.94371197|  0.85054907|  0.65702479|  0.74136057|
|10 |0.94827586|  0.84763603|  0.70523416|  0.76959877|
|20 |0.9759973 |  0.94208554|  0.85674931|  0.89734288|

Contrary to my previous thought, larger margin -> better results. Still, I think it is because large margin brings better convergence.

## 20180724

Mini-batch is now supported. It improves training speed significantly. 
Not only the number of epochs required for convergence is drastically reduced,
the training efficiency of an epoch is also greatly reduced. 

I'll test the embedding result tomorrow.
>>>>>>> cc68200b397288404dd273562d84a5916268216d
